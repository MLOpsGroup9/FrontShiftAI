from chat_pipeline.rag.pipeline import RAGPipeline
from chat_pipeline.rag.generator import load_llm, generation

def main():
    print("\nüîµ Interactive RAG CLI (Local LLaMA ‚Üí Mercury Fallback)")
    print("Type 'exit' to quit.\n")

    pipeline = RAGPipeline()

    # Load LLaMA once
    try:
        llm = load_llm()
        print("ü¶ô Using local LLaMA model\n")
    except Exception as e:
        print(f"‚ö†Ô∏è Local LLaMA unavailable: {e}")
        print("‚û°Ô∏è  Falling back to Mercury.\n")
        llm = None

    while True:
        query = input("You: ").strip()
        if query.lower() in {"exit", "quit"}:
            print("üëã Bye!")
            break

        # ---- STEP 1: Do retrieval only ----
        docs, metadatas = pipeline._run_retrieval_only(query=query)

        # ---- STEP 2: Use generator manually ----
        answer, sources = generation(
            query=query,
            documents=docs,
            metadatas=metadatas,
            llm=llm,     # <-- cached LLaMA or Mercury fallback
            stream=False
        )

        print("\nAssistant:\n", answer)
        print("\nSources:")
        for s in sources:
            print(f"- {s}")
        print("\n" + "-"*60 + "\n")

if __name__ == "__main__":
    main()
