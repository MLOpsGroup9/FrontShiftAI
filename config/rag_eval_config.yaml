# RAG Evaluation Configuration

# Model paths
models:
  llm_path: models/mistral-7b-instruct-v0.2.Q4_K_M.gguf
  embedding_model: sentence-transformers/all-MiniLM-L6-v2
  n_ctx: 4096
  n_threads: 4
  temperature: 0.7

# ChromaDB settings
chromadb:
  persist_dir: data/vector_db/chroma
  collection_name: handbook_chunks

# Hybrid search
hybrid_search:
  enabled: true
  bm25_index_path: data/vector_db/bm25/handbook_chunks
  bm25_weight: 0.3
  semantic_weight: 0.7

# Evaluation queries
evaluation:
  query_file: data/eval/sample_test_queries.jsonl
  generate_synthetic: true
  synthetic_count: 10
  
# Retrieval metrics
retrieval:
  k_values: [1, 3, 5, 10]
  min_relevance_threshold: 0.5

# Latency benchmarks
latency:
  num_runs: 10
  warmup_runs: 2
  target_retrieval_ms: 100
  target_llm_ms: 2000

# Embedding health
embedding_health:
  min_intra_similarity: 0.5
  max_inter_similarity: 0.7
  sample_size: 50

# Output
output:
  report_dir: reports/rag_eval_reports
  save_baseline: true
  baseline_path: reports/rag_eval_reports/baseline.json

