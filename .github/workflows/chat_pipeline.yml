# This workflow only runs on the main branch
# It won't trigger automatically on feature branches
# To test manually, use the "Run workflow" button after configuring secrets
# See ml_pipeline/ci_cd/docs/SECRETS_SETUP.md for secret configuration

name: Chat Pipeline - Train, Validate, and Deploy

on:
  push:
    branches: [ main, raghav-branch ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

# Prevent parallel runs on the same branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  setup-and-validation:
    name: Setup & Validation
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: 'pip'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r chat_pipeline/requirements.txt
          pip install pyyaml  # For config file parsing

      - name: Set Environment Variables
        run: |
          echo "PYTHONPATH=$(pwd)" >> $GITHUB_ENV
          echo "TOKENIZERS_PARALLELISM=false" >> $GITHUB_ENV
        shell: bash

      - name: Validate Environment
        run: |
          python ml_pipeline/ci_cd/validate_environment.py --verbose
        continue-on-error: false

      - name: Set W&B Environment Variables
        run: |
          echo "WANDB_API_KEY=${{ secrets.WANDB_API_KEY }}" >> $GITHUB_ENV
          echo "WANDB_ENTITY=group9mlops-northeastern-university" >> $GITHUB_ENV
          echo "WANDB_PROJECT=FrontShiftAI" >> $GITHUB_ENV
        shell: bash

  run-evaluation:
    name: Run Evaluation Pipeline
    runs-on: ubuntu-latest
    needs: setup-and-validation
    timeout-minutes: 30

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: 'pip'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r chat_pipeline/requirements.txt

      - name: Set Environment Variables
        run: |
          echo "PYTHONPATH=$(pwd)" >> $GITHUB_ENV
          echo "TOKENIZERS_PARALLELISM=false" >> $GITHUB_ENV
        shell: bash

      - name: Run Evaluation Pipeline
        env:
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
          WANDB_ENTITY: group9mlops-northeastern-university
          WANDB_PROJECT: FrontShiftAI
        run: |
          python ml_pipeline/eval_pipeline_runner.py

      - name: Verify Evaluation Results
        run: |
          if [ ! -f "ml_pipeline/evaluation/eval_results/unified_summary.json" ]; then
            echo "❌ unified_summary.json not found"
            exit 1
          fi
          echo "✅ Evaluation results generated successfully"

      - name: Upload Evaluation Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: eval-results
          path: |
            ml_pipeline/evaluation/eval_results/
            models/
          retention-days: 30

      - name: Output Metrics
        id: metrics
        run: |
          python3 << 'EOF'
          import json
          import pathlib
          
          summary_path = pathlib.Path('ml_pipeline/evaluation/eval_results/unified_summary.json')
          with open(summary_path, 'r') as f:
              data = json.load(f)
          
          mean_sim = data['rag']['mean_semantic_sim']
          mean_prec = data['rag']['mean_precision_at_k']
          
          print(f"mean_semantic_sim={mean_sim}")
          print(f"mean_precision_at_k={mean_prec}")
          EOF
        continue-on-error: true

  quality-gate-check:
    name: Quality Gate Check
    runs-on: ubuntu-latest
    needs: run-evaluation
    timeout-minutes: 5

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: 'pip'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r chat_pipeline/requirements.txt
          pip install pyyaml

      - name: Set Environment Variables
        run: |
          echo "PYTHONPATH=$(pwd)" >> $GITHUB_ENV
        shell: bash

      - name: Download Evaluation Artifacts
        uses: actions/download-artifact@v4
        with:
          name: eval-results
          path: .

      - name: Determine Environment
        id: env
        run: |
          if [ "${{ github.ref }}" == "refs/heads/main" ]; then
            echo "environment=production" >> $GITHUB_OUTPUT
          elif [ "${{ github.ref }}" == "refs/heads/dev" ]; then
            echo "environment=staging" >> $GITHUB_OUTPUT
          else
            echo "environment=development" >> $GITHUB_OUTPUT
          fi

      - name: Run Quality Gate Check
        id: quality_gate
        run: |
          python ml_pipeline/ci_cd/quality_gate_checker.py \
            --environment ${{ steps.env.outputs.environment }} \
            --output-json > quality_gate_result.json
          
          # Extract results
          DEPLOYMENT_APPROVED=$(python3 -c "import json; print(json.load(open('quality_gate_result.json'))['deployment_approved'])")
          QUALITY_STATUS=$(python3 -c "import json; print(json.load(open('quality_gate_result.json'))['status'])")
          
          echo "deployment_approved=$DEPLOYMENT_APPROVED" >> $GITHUB_OUTPUT
          echo "quality_gate_status=$QUALITY_STATUS" >> $GITHUB_OUTPUT
          
          if [ "$DEPLOYMENT_APPROVED" != "True" ]; then
            echo "❌ Quality gates failed. Deployment not approved."
            exit 1
          fi
          echo "✅ Quality gates passed. Deployment approved."

      - name: Upload Quality Gate Results
        uses: actions/upload-artifact@v4
        with:
          name: quality-gate-results
          path: quality_gate_result.json
          retention-days: 30

  deploy-model:
    name: Deploy Model to Registry
    runs-on: ubuntu-latest
    needs: [run-evaluation, quality-gate-check]
    if: needs.quality-gate-check.outputs.deployment_approved == 'True'
    timeout-minutes: 10

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: 'pip'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r chat_pipeline/requirements.txt

      - name: Set Environment Variables
        run: |
          echo "PYTHONPATH=$(pwd)" >> $GITHUB_ENV
        shell: bash

      - name: Download Evaluation Artifacts
        uses: actions/download-artifact@v4
        with:
          name: eval-results
          path: .

      - name: Download Quality Gate Results
        uses: actions/download-artifact@v4
        with:
          name: quality-gate-results
          path: .

      - name: Get Quality Gate Status
        id: qg_status
        run: |
          QUALITY_STATUS=$(python3 -c "import json; print(json.load(open('quality_gate_result.json'))['status'])")
          echo "quality_gate_status=$QUALITY_STATUS" >> $GITHUB_OUTPUT

      - name: Deploy Model to Registry
        run: |
          python ml_pipeline/ci_cd/deploy_model.py \
            --quality-gate-status ${{ steps.qg_status.outputs.quality_gate_status }}

      - name: Upload Model Registry
        uses: actions/upload-artifact@v4
        with:
          name: model-registry
          path: models_registry/
          retention-days: 90

  notify:
    name: Send Email Notification
    runs-on: ubuntu-latest
    needs: [setup-and-validation, run-evaluation, quality-gate-check, deploy-model]
    if: always()
    timeout-minutes: 5

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: 'pip'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r chat_pipeline/requirements.txt

      - name: Download Evaluation Artifacts
        uses: actions/download-artifact@v4
        with:
          name: eval-results
          path: .
        continue-on-error: true

      - name: Download Quality Gate Results
        uses: actions/download-artifact@v4
        with:
          name: quality-gate-results
          path: .
        continue-on-error: true

      - name: Send Email Notification
        env:
          EMAIL_SENDER: ${{ secrets.EMAIL_SENDER }}
          EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
          EMAIL_RECEIVER: ${{ secrets.EMAIL_RECEIVER }}
          SETUP_STATUS: ${{ needs.setup-and-validation.result }}
          EVAL_STATUS: ${{ needs.run-evaluation.result }}
          QG_STATUS: ${{ needs.quality-gate-check.result }}
          DEPLOY_STATUS: ${{ needs.deploy-model.result }}
          GITHUB_RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
        run: |
          python3 << 'EOF'
          import os
          import json
          import pathlib
          import smtplib
          from email.mime.text import MIMEText
          from email.mime.multipart import MIMEMultipart
          from email.mime.text import MIMEText
          
          # Determine overall status
          setup_status = os.getenv('SETUP_STATUS', 'unknown')
          eval_status = os.getenv('EVAL_STATUS', 'unknown')
          qg_status = os.getenv('QG_STATUS', 'unknown')
          deploy_status = os.getenv('DEPLOY_STATUS', 'skipped')
          
          if all(s == 'success' for s in [setup_status, eval_status, qg_status]) and deploy_status in ['success', 'skipped']:
              overall_status = 'SUCCESS ✅'
              subject = 'FrontShiftAI CI/CD Pipeline - SUCCESS'
          else:
              overall_status = 'FAILURE ❌'
              subject = 'FrontShiftAI CI/CD Pipeline - FAILURE'
          
          # Load metrics if available
          mean_sim = 'N/A'
          mean_precision = 'N/A'
          quality_gate_status = 'N/A'
          
          summary_path = pathlib.Path('ml_pipeline/evaluation/eval_results/unified_summary.json')
          if summary_path.exists():
              with open(summary_path, 'r') as f:
                  data = json.load(f)
                  mean_sim = data['rag']['mean_semantic_sim']
                  mean_precision = data['rag']['mean_precision_at_k']
          
          qg_result_path = pathlib.Path('quality_gate_result.json')
          if qg_result_path.exists():
              with open(qg_result_path, 'r') as f:
                  qg_data = json.load(f)
                  quality_gate_status = qg_data.get('status', 'N/A')
          
          # Create email body
          body = f"""FrontShiftAI CI/CD Pipeline Run Summary
          
          Repository: {os.getenv('GITHUB_REPOSITORY', 'N/A')}
          Branch: {os.getenv('GITHUB_REF_NAME', 'N/A')}
          Workflow: {os.getenv('GITHUB_WORKFLOW', 'N/A')}
          Run ID: {os.getenv('GITHUB_RUN_ID', 'N/A')}
          Commit: {os.getenv('GITHUB_SHA', 'N/A')[:7]}
          
          ═══════════════════════════════════════
          JOB STATUS
          ═══════════════════════════════════════
          • Setup & Validation:     {setup_status.upper()}
          • Run Evaluation:         {eval_status.upper()}
          • Quality Gate Check:     {qg_status.upper()}
          • Deploy Model:           {deploy_status.upper()}
          
          Overall Status: {overall_status}
          
          ═══════════════════════════════════════
          EVALUATION METRICS
          ═══════════════════════════════════════
          • Mean Semantic Similarity: {mean_sim}
          • Mean Precision@K:         {mean_precision}
          • Quality Gate Status:       {quality_gate_status}
          
          ═══════════════════════════════════════
          
          View full run details:
          {os.getenv('GITHUB_RUN_URL', 'N/A')}
          
          ---
          This is an automated message from FrontShiftAI CI/CD Pipeline.
          """
          
          # Send email
          sender = os.getenv('EMAIL_SENDER')
          password = os.getenv('EMAIL_PASSWORD')
          receiver = os.getenv('EMAIL_RECEIVER')
          
          if not all([sender, password, receiver]):
              print('Email credentials not configured. Skipping email notification.')
              exit(0)
          
          msg = MIMEMultipart()
          msg['From'] = sender
          msg['To'] = receiver
          msg['Subject'] = subject
          msg.attach(MIMEText(body, 'plain'))
          
          try:
              with smtplib.SMTP_SSL('smtp.gmail.com', 465, timeout=15) as server:
                  server.login(sender, password)
                  server.send_message(msg)
              print(f'✅ Email sent successfully to {receiver}')
          except Exception as e:
              print(f'❌ Failed to send email: {e}')
              exit(0)  # Don't fail the workflow if email fails
          EOF
