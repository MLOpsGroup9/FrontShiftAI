name: Chat Pipeline - Mac Self-Hosted Check

on:
  workflow_dispatch:

jobs:
  mac-self-hosted:
    runs-on: [self-hosted, macos]
    timeout-minutes: 120

    env:
      LLAMA_MODEL_PATH: /Users/sriks/Documents/Projects/FrontShiftAI/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf
      GENERATION_BACKEND: local
      TOKENIZERS_PARALLELISM: false
      CHAT_PIPELINE_ALLOW_HEAVY_FALLBACKS: 1

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Set up Python (system)
        run: |
          echo "Using system Python:"
          which python3
          python3 --version
        shell: bash

      - name: Install Dependencies
        run: |
          python3 -m pip install --upgrade pip
          pip3 install -r chat_pipeline/requirements.txt
          pip3 install llama-cpp-python
          pip3 install pyyaml
        shell: bash

      - name: Export PYTHONPATH
        run: |
          echo "PYTHONPATH=$(pwd)" >> $GITHUB_ENV

      - name: Auth to GCP
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up gcloud SDK (gsutil)
        uses: google-github-actions/setup-gcloud@v2

      - name: Sync vector DB from GCS
        run: |
          mkdir -p chat_pipeline/data_pipeline/data/vector_db
          gsutil -m rsync -r gs://frontshiftai-data/data/vector_db chat_pipeline/data_pipeline/data/vector_db
          echo "CHROMA_DIR=chat_pipeline/data_pipeline/data/vector_db" >> $GITHUB_ENV
        shell: bash

      - name: Set W&B Environment Variables
        run: |
          echo "WANDB_ENTITY=group9mlops-northeastern-university" >> $GITHUB_ENV
          echo "WANDB_PROJECT=FrontShiftAI" >> $GITHUB_ENV

      - name: Generate Core Evaluation Responses
        env:
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
        run: |
          python3 -m chat_pipeline.cli --config chat_pipeline/configs/experiments/core_eval.yaml --mode generate --debug

      - name: Judge Core Evaluation Responses
        env:
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
        run: |
          python3 -m chat_pipeline.cli --config chat_pipeline/configs/experiments/core_eval.yaml --mode judge --debug || true

      - name: Verify Core Eval (Relaxed)
        run: |
          echo "üîç Listing evaluation results:"
          ls -R chat_pipeline/results || true

          if [ -f "chat_pipeline/results/examples.jsonl" ] || \
             [ -f "chat_pipeline/results/summary.json" ]; then
              echo "‚úÖ Core eval produced output files."
              exit 0
          fi

          echo "‚ùå No eval output detected ‚Äî failing."
          exit 1

      - name: Generate Full Evaluation Responses
        env:
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
        run: |
          python3 -m chat_pipeline.cli --config chat_pipeline/configs/experiments/full_eval.yaml --mode generate --debug

      - name: Judge Full Evaluation Responses
        env:
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
        run: |
          python3 -m chat_pipeline.cli --config chat_pipeline/configs/experiments/full_eval.yaml --mode judge --debug || true

      - name: Verify Full Eval (Relaxed)
        run: |
          echo "üîç Listing evaluation results:"
          ls -R chat_pipeline/results || true

          if [ -f "chat_pipeline/results/examples.jsonl" ] || \
             [ -f "chat_pipeline/results/summary.json" ]; then
              echo "‚úÖ Full eval produced output files."
              exit 0
          fi

          echo "‚ùå No eval output detected ‚Äî failing."
          exit 1

      - name: Upload Evaluation Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: self-hosted-eval-results
          path: |
            chat_pipeline/results/
            models/
          retention-days: 30
