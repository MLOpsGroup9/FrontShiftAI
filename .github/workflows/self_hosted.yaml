name: Chat Pipeline - Mac Self-Hosted Check

on:
  workflow_dispatch:

jobs:
  mac-self-hosted:
    runs-on: [self-hosted, macos]
    timeout-minutes: 120

    env:
      TOKENIZERS_PARALLELISM: false
      CHAT_PIPELINE_ALLOW_HEAVY_FALLBACKS: 1

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Python Info
        run: |
          which python3
          python3 --version

      - name: Install Dependencies
        run: |
          python3 -m pip install --upgrade pip
          pip3 install -r chat_pipeline/requirements.txt
          CMAKE_ARGS="-DLLAMA_ACCELERATE=on" pip3 install --force-reinstall --upgrade llama-cpp-python
          pip3 install pyyaml

      - name: Set Environment
        run: |
          echo "LLAMA_MODEL_PATH=/Users/sriks/Documents/Projects/FrontShiftAI/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf" >> $GITHUB_ENV
          echo "GENERATION_BACKEND=local" >> $GITHUB_ENV
          echo "PYTHONPATH=$(pwd)" >> $GITHUB_ENV

      - name: Test LLaMA
        run: |
          python3 - << 'EOF'
          from llama_cpp import Llama
          import os
          Llama(model_path=os.getenv("LLAMA_MODEL_PATH"))
          print("Model OK")
          EOF

      - name: GCP Auth
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v2

      - name: Sync Vector DB
        run: |
          mkdir -p chat_pipeline/data_pipeline/data/vector_db
          gsutil -m -o "GSUtil:parallel_process_count=1" \
            rsync -r gs://frontshiftai-data/data/vector_db \
            chat_pipeline/data_pipeline/data/vector_db
          echo "CHROMA_DIR=chat_pipeline/data_pipeline/data/vector_db" >> $GITHUB_ENV

      - name: Set W&B Env
        run: |
          echo "WANDB_ENTITY=group9mlops-northeastern-university" >> $GITHUB_ENV
          echo "WANDB_PROJECT=FrontShiftAI" >> $GITHUB_ENV

      # ------------------------------------------------------------------
      # Core Eval Generate
      # ------------------------------------------------------------------
      - name: Core Eval Generate
        env:
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
        run: |
          python3 -m chat_pipeline.cli \
            --config chat_pipeline/configs/experiments/core_eval.yaml \
            --mode generate --debug

      # ------------------------------------------------------------------
      # Core Eval Judge  (corrected; does NOT break summaries)
      # ------------------------------------------------------------------
      - name: Core Eval Judge
        env:
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
        run: |
          mkdir -p chat_pipeline/results

          # Run judge normally so chat_pipeline can complete its internal lifecycle
          python3 -m chat_pipeline.cli \
            --config chat_pipeline/configs/experiments/core_eval.yaml \
            --mode judge --debug \
            > raw_core_judge_output.txt 2>&1 || true

          # Extract JSON cleanly AFTER the judge finishes
          RAW=$(cat raw_core_judge_output.txt)
          CLEAN=$(echo "$RAW" \
            | sed 's/```json//g' \
            | sed 's/```//g' \
            | sed 's/\*\*\*//g'
          )
          JSON=$(echo "$CLEAN" | perl -0777 -ne 'print "$1" if /(\{(?:[^{}]|(?1))*\})/s')

          echo "$JSON" > chat_pipeline/results/fixed_core_judge.json
          echo "Core judge done"

      # ------------------------------------------------------------------
      # Verify Core Eval Summaries
      # ------------------------------------------------------------------
      - name: Verify Core Eval
        run: |
          test -f chat_pipeline/results/core_experiment_summary.json
          test -f chat_pipeline/results/core_eval_main/summary.json
          test -f chat_pipeline/results/core_eval_slices/summary.json
          echo "Core eval OK"

      # ------------------------------------------------------------------
      # Full Eval Generate
      # ------------------------------------------------------------------
      - name: Full Eval Generate
        env:
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
        run: |
          python3 -m chat_pipeline.cli \
            --config chat_pipeline/configs/experiments/full_eval.yaml \
            --mode generate --debug

      # ------------------------------------------------------------------
      # Full Eval Judge  (same safe pattern)
      # ------------------------------------------------------------------
      - name: Full Eval Judge
        env:
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
        run: |
          mkdir -p chat_pipeline/results

          python3 -m chat_pipeline.cli \
            --config chat_pipeline/configs/experiments/full_eval.yaml \
            --mode judge --debug \
            > raw_full_judge_output.txt 2>&1 || true

          RAW=$(cat raw_full_judge_output.txt)
          CLEAN=$(echo "$RAW" \
            | sed 's/```json//g' \
            | sed 's/```//g' \
            | sed 's/\*\*\*//g'
          )
          JSON=$(echo "$CLEAN" | perl -0777 -ne 'print "$1" if /(\{(?:[^{}]|(?1))*\})/s')

          echo "$JSON" > chat_pipeline/results/fixed_full_judge.json
          echo "Full judge done"

      # ------------------------------------------------------------------
      # Verify Full Eval Summaries
      # ------------------------------------------------------------------
      - name: Verify Full Eval
        run: |
          test -f chat_pipeline/results/experiment_summary.json
          test -f chat_pipeline/results/eval_main/summary.json
          echo "Full eval OK"

      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: self-hosted-eval-results
          path: chat_pipeline/results/
          retention-days: 30
