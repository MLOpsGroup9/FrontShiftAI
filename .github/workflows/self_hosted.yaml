name: Chat Pipeline - Mac Self-Hosted Check

on:
  workflow_dispatch:

jobs:
  mac-self-hosted:
    runs-on: [self-hosted, macos]
    timeout-minutes: 120

    env:
      TOKENIZERS_PARALLELISM: false
      CHAT_PIPELINE_ALLOW_HEAVY_FALLBACKS: 1

    steps:
      # -------------------------
      # Checkout Repository
      # -------------------------
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          lfs: true

      # -------------------------
      # Use system Python
      # -------------------------
      - name: Set up Python (system)
        run: |
          echo "Using system Python:"
          which python3
          python3 --version
        shell: bash

      # -------------------------
      # Install Python Dependencies
      # -------------------------
      - name: Install Dependencies
        run: |
          python3 -m pip install --upgrade pip
          pip3 install -r chat_pipeline/requirements.txt
          
          echo "Installing llama-cpp-python with Accelerate..."
          CMAKE_ARGS="-DLLAMA_ACCELERATE=on" pip3 install --force-reinstall --upgrade llama-cpp-python

          pip3 install pyyaml
        shell: bash

      # -------------------------
      # Export LLaMA Environment Variables
      # -------------------------
      - name: Export LLaMA env vars
        run: |
          echo "LLAMA_MODEL_PATH=/Users/sriks/Documents/Projects/FrontShiftAI/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf" >> $GITHUB_ENV
          echo "GENERATION_BACKEND=local" >> $GITHUB_ENV
          echo "PYTHONPATH=$(pwd)" >> $GITHUB_ENV
        shell: bash

      # -------------------------
      # Debug: Test if LLaMA loads
      # -------------------------
      - name: Debug - Verify LLaMA load
        run: |
          python3 - << 'EOF'
          from llama_cpp import Llama
          import os

          model_path = os.getenv("LLAMA_MODEL_PATH")
          print("LLAMA_MODEL_PATH =", model_path)

          try:
              llm = Llama(model_path=model_path)
              print("‚úÖ LLaMA model loaded successfully!")
          except Exception as e:
              print("‚ùå LLaMA load failed:", e)
              raise
          EOF

      # -------------------------
      # Authenticate to GCP
      # -------------------------
      - name: Auth to GCP
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      # -------------------------
      # Set up gcloud
      # -------------------------
      - name: Set up gcloud SDK (gsutil)
        uses: google-github-actions/setup-gcloud@v2

      # -------------------------
      # Sync Vector DB
      # -------------------------
      - name: Sync vector DB from GCS
        run: |
          mkdir -p chat_pipeline/data_pipeline/data/vector_db
          gsutil -m rsync -r gs://frontshiftai-data/data/vector_db chat_pipeline/data_pipeline/data/vector_db
          echo "CHROMA_DIR=chat_pipeline/data_pipeline/data/vector_db" >> $GITHUB_ENV
        shell: bash

      # -------------------------
      # Weights & Biases ENV SETUP
      # -------------------------
      - name: Set W&B Environment Variables
        run: |
          echo "WANDB_ENTITY=group9mlops-northeastern-university" >> $GITHUB_ENV
          echo "WANDB_PROJECT=FrontShiftAI" >> $GITHUB_ENV
        shell: bash

      # -------------------------
      # Core Evaluation Generation
      # -------------------------
      - name: Generate Core Evaluation Responses
        env:
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
        run: |
          python3 -m chat_pipeline.cli --config chat_pipeline/configs/experiments/core_eval.yaml --mode generate --debug

      # -------------------------
      # Core Evaluation Judging (CLEAN JSON WRAPPER)
      # -------------------------
      - name: Judge Core Evaluation Responses
        env:
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
        run: |
          set +e
          echo "üìå Running judge step..."

          RAW_OUTPUT=$(python3 -m chat_pipeline.cli \
            --config chat_pipeline/configs/experiments/core_eval.yaml \
            --mode judge --debug 2>&1)

          EXIT_CODE=$?

          echo "---------------- RAW JUDGE OUTPUT ----------------"
          echo "$RAW_OUTPUT"
          echo "--------------------------------------------------"

          echo "üìå Cleaning judge JSON..."

          CLEANED=$(echo "$RAW_OUTPUT" \
            | sed 's/```json//g' \
            | sed 's/```//g' \
            | sed 's/\*\*\*//g' \
            | sed 's/^[[:space:]]*//g'
          )

          JSON_BLOCK=$(echo "$CLEANED" | perl -0777 -ne 'print "$1\n" if /(\{(?:[^{}]|(?1))*\})/s')

          if [ -z "$JSON_BLOCK" ]; then
            echo "‚ùå Could not extract JSON from judge output."
            echo "Judge step EXIT CODE: $EXIT_CODE"
            exit $EXIT_CODE
          fi

          echo "$JSON_BLOCK" > chat_pipeline/results/fixed_core_judge.json
          echo "üìå Cleaned JSON written to chat_pipeline/results/fixed_core_judge.json"

          if [ $EXIT_CODE -ne 0 ]; then
            echo "‚ö†Ô∏è Judge exited with code $EXIT_CODE, but JSON was successfully extracted. Continuing..."
          else
            echo "‚úÖ Judge completed with valid JSON extracted."
          fi

      # -------------------------
      # Validate Core Eval Outputs
      # -------------------------
      - name: Verify Core Evaluation Results
        run: |
          if [ ! -f "chat_pipeline/results/core_experiment_summary.json" ]; then
            echo "‚ùå core_experiment_summary.json missing"
            exit 1
          fi
          if [ ! -f "chat_pipeline/results/core_eval_main/summary.json" ]; then
            echo "‚ùå core_eval_main/summary.json missing"
            exit 1
          fi
          if [ ! -f "chat_pipeline/results/core_eval_slices/summary.json" ]; then
            echo "‚ùå core_eval_slices/summary.json missing"
            exit 1
          fi
          echo "‚úÖ Core eval results ready"

      # -------------------------
      # FULL Evaluation Pipeline
      # -------------------------
      - name: Generate Full Evaluation Responses
        env:
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
        run: |
          python3 -m chat_pipeline.cli --config chat_pipeline/configs/experiments/full_eval.yaml --mode generate --debug

      - name: Judge Full Evaluation Responses
        env:
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
        run: |
          set +e
          echo "üìå Running FULL evaluation judge step..."

          RAW_OUTPUT=$(python3 -m chat_pipeline.cli \
            --config chat_pipeline/configs/experiments/full_eval.yaml \
            --mode judge --debug 2>&1)

          EXIT_CODE=$?

          echo "---------------- RAW JUDGE OUTPUT ----------------"
          echo "$RAW_OUTPUT"
          echo "--------------------------------------------------"

          echo "üìå Cleaning judge JSON..."

          CLEANED=$(echo "$RAW_OUTPUT" \
            | sed 's/```json//g' \
            | sed 's/```//g' \
            | sed 's/\*\*\*//g' \
            | sed 's/^[[:space:]]*//g'
          )

          JSON_BLOCK=$(echo "$CLEANED" | perl -0777 -ne 'print "$1\n" if /(\{(?:[^{}]|(?1))*\})/s')

          if [ -z "$JSON_BLOCK" ]; then
            echo "‚ùå Could not extract JSON from judge output."
            echo "Judge step EXIT CODE: $EXIT_CODE"
            exit $EXIT_CODE
          fi

          echo "$JSON_BLOCK" > chat_pipeline/results/fixed_full_judge.json
          echo "üìå Cleaned JSON written to chat_pipeline/results/fixed_full_judge.json"

          if [ $EXIT_CODE -ne 0 ]; then
            echo "‚ö†Ô∏è Judge exited with code $EXIT_CODE, but JSON was successfully extracted. Continuing..."
          else
            echo "‚úÖ FULL Judge completed with valid JSON extracted."
          fi

      - name: Verify Evaluation Results
        run: |
          if [ ! -f "chat_pipeline/results/experiment_summary.json" ]; then
            echo "‚ùå experiment_summary.json missing"
            exit 1
          fi
          if [ ! -f "chat_pipeline/results/eval_main/summary.json" ]; then
            echo "‚ùå eval_main/summary.json missing"
            exit 1
          fi
          echo "‚úÖ Full evaluation results generated successfully"

      # -------------------------
      # Upload Artifacts
      # -------------------------
      - name: Upload Evaluation Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: self-hosted-eval-results
          path: |
            chat_pipeline/results/
            models/
          retention-days: 30
