name: RAG Pipeline - Evaluation & Deployment

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  setup-and-validation:
    name: Setup & Environment Validation
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: 'pip'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pyyaml

      - name: Set Environment Variables
        run: |
          echo "PYTHONPATH=$(pwd)" >> $GITHUB_ENV
          echo "TOKENIZERS_PARALLELISM=false" >> $GITHUB_ENV

      - name: Validate Environment
        run: |
          python chat_pipeline/ci_cd/validate_environment.py \
            --verbose \
            --test-dir chat_pipeline/evaluation/test_questions \
            --quality-gates-config chat_pipeline/configs/quality_gates.yml

  run-evaluation:
    name: Run Evaluation Pipeline
    runs-on: ubuntu-latest
    needs: setup-and-validation
    timeout-minutes: 30

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: 'pip'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Set Environment Variables
        run: |
          echo "PYTHONPATH=$(pwd)" >> $GITHUB_ENV
          echo "TOKENIZERS_PARALLELISM=false" >> $GITHUB_ENV
          echo "WANDB_API_KEY=${{ secrets.WANDB_API_KEY }}" >> $GITHUB_ENV
          echo "WANDB_ENTITY=group9mlops-northeastern-university" >> $GITHUB_ENV
          echo "WANDB_PROJECT=FrontShiftAI" >> $GITHUB_ENV
          echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY || '' }}" >> $GITHUB_ENV
          echo "MLFLOW_TRACKING_URI=${{ secrets.MLFLOW_TRACKING_URI || 'file:./mlruns' }}" >> $GITHUB_ENV

      - name: Create Timestamped Output Directory
        id: output_dir
        run: |
          OUTPUT_DIR="chat_pipeline/evaluation/eval_results/run_$(date +%Y%m%d_%H%M%S)"
          echo "output_dir=$OUTPUT_DIR" >> $GITHUB_OUTPUT
          echo "Evaluation results will be saved to: $OUTPUT_DIR"

      - name: Run Evaluation Runner
        run: |
          echo "Starting RAG evaluation pipeline..."
          python chat_pipeline/evaluation/evaluation_runner.py \
            --test-dir chat_pipeline/evaluation/test_questions \
            --output-dir ${{ steps.output_dir.outputs.output_dir }} \
            --wandb-project FrontShiftAI \
            --wandb-entity group9mlops-northeastern-university
          
          echo "Evaluation complete"

      - name: Verify Evaluation Results
        run: |
          OUTPUT_DIR="${{ steps.output_dir.outputs.output_dir }}"
          SUMMARY_PATH="$OUTPUT_DIR/summary.json"
          
          if [ ! -f "$SUMMARY_PATH" ]; then
            echo "ERROR: summary.json not found at $SUMMARY_PATH"
            exit 1
          fi
          
          echo "SUCCESS: Found summary.json"
          echo "Evaluation Summary:"
          
          python3 << 'EOF'
          import json
          import sys
          
          summary_path = "${{ steps.output_dir.outputs.output_dir }}/summary.json"
          
          try:
              with open(summary_path, 'r') as f:
                  data = json.load(f)
              
              print("\nQuality Metrics (0-5 scale):")
              if 'average_scores' in data:
                  for metric, value in data['average_scores'].items():
                      print(f"  {metric}: {value:.3f}")
              
              print("\nPerformance Metrics:")
              if 'performance' in data:
                  for metric, value in data['performance'].items():
                      if isinstance(value, (int, float)):
                          print(f"  {metric}: {value:.3f}")
              
              print(f"\nTotal examples: {data.get('metadata', {}).get('total_examples', 'N/A')}")
          
          except Exception as e:
              print(f"Error reading summary: {e}")
              sys.exit(1)
          EOF

      - name: Upload Evaluation Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: eval-results
          path: |
            chat_pipeline/evaluation/eval_results/
            mlruns/
          retention-days: 30

  quality-gate-check:
    name: Quality Gate Validation
    runs-on: ubuntu-latest
    needs: run-evaluation
    timeout-minutes: 5
    
    outputs:
      deployment_approved: ${{ steps.quality_gate.outputs.deployment_approved }}
      quality_status: ${{ steps.quality_gate.outputs.quality_status }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: 'pip'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pyyaml

      - name: Set Environment Variables
        run: |
          echo "PYTHONPATH=$(pwd)" >> $GITHUB_ENV

      - name: Download Evaluation Artifacts
        uses: actions/download-artifact@v4
        with:
          name: eval-results
          path: chat_pipeline/evaluation/eval_results/

      - name: Determine Environment
        id: env
        run: |
          if [ "${{ github.ref }}" == "refs/heads/main" ]; then
            echo "environment=production" >> $GITHUB_OUTPUT
          elif [ "${{ github.ref }}" == "refs/heads/dev" ]; then
            echo "environment=staging" >> $GITHUB_OUTPUT
          else
            echo "environment=development" >> $GITHUB_OUTPUT
          fi
          
          echo "Environment: $(cat $GITHUB_OUTPUT | grep environment | cut -d'=' -f2)"

      - name: Run Quality Gate Check
        id: quality_gate
        run: |
          echo "Running quality gate checks..."
          
          python chat_pipeline/ci_cd/quality_gate_checker.py \
            --environment ${{ steps.env.outputs.environment }} \
            --config chat_pipeline/configs/quality_gates.yml \
            --results-dir chat_pipeline/evaluation/eval_results \
            --output chat_pipeline/evaluation/eval_results/quality_gate_result.json
          
          DEPLOYMENT_APPROVED=$(python3 -c "import json; print(json.load(open('chat_pipeline/evaluation/eval_results/quality_gate_result.json'))['deployment_approved'])")
          QUALITY_STATUS=$(python3 -c "import json; print(json.load(open('chat_pipeline/evaluation/eval_results/quality_gate_result.json'))['status'])")
          
          echo "deployment_approved=$DEPLOYMENT_APPROVED" >> $GITHUB_OUTPUT
          echo "quality_status=$QUALITY_STATUS" >> $GITHUB_OUTPUT
          
          echo "Quality Gate Status: $QUALITY_STATUS"
          echo "Deployment Approved: $DEPLOYMENT_APPROVED"
        continue-on-error: false

      - name: Upload Quality Gate Results
        uses: actions/upload-artifact@v4
        with:
          name: quality-gate-results
          path: chat_pipeline/evaluation/eval_results/quality_gate_result.json
          retention-days: 30

  notify:
    name: Send Notification
    runs-on: ubuntu-latest
    needs: [setup-and-validation, run-evaluation, quality-gate-check]
    if: always()
    timeout-minutes: 5

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Download Artifacts
        uses: actions/download-artifact@v4
        with:
          name: eval-results
          path: chat_pipeline/evaluation/eval_results/
        continue-on-error: true

      - name: Download Quality Gate Results
        uses: actions/download-artifact@v4
        with:
          name: quality-gate-results
          path: .
        continue-on-error: true

      - name: Send Email Notification
        env:
          EMAIL_SENDER: ${{ secrets.EMAIL_SENDER }}
          EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
          EMAIL_RECEIVER: ${{ secrets.EMAIL_RECEIVER }}
          SETUP_STATUS: ${{ needs.setup-and-validation.result }}
          EVAL_STATUS: ${{ needs.run-evaluation.result }}
          QG_STATUS: ${{ needs.quality-gate-check.result }}
          GITHUB_RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
        run: |
          python3 << 'EOF'
          import os
          import json
          import pathlib
          import smtplib
          from email.mime.text import MIMEText
          from email.mime.multipart import MIMEMultipart
          from glob import glob
          
          setup = os.getenv('SETUP_STATUS', 'unknown')
          evaluation = os.getenv('EVAL_STATUS', 'unknown')
          quality_gate = os.getenv('QG_STATUS', 'unknown')
          
          if all(s == 'success' for s in [setup, evaluation, quality_gate]):
              overall = 'SUCCESS'
              subject = 'FrontShiftAI CI/CD Pipeline - SUCCESS'
          else:
              overall = 'FAILURE'
              subject = 'FrontShiftAI CI/CD Pipeline - FAILURE'
          
          metrics_summary = "Metrics not available"
          qg_status = "N/A"
          
          summary_files = glob('chat_pipeline/evaluation/eval_results/*/summary.json')
          if summary_files:
              latest_summary = max(summary_files, key=os.path.getmtime)
              try:
                  with open(latest_summary, 'r') as f:
                      data = json.load(f)
                  
                  avg_scores = data.get('average_scores', {})
                  perf = data.get('performance', {})
                  
                  metrics_summary = f"""
          Precision: {avg_scores.get('precision', 'N/A')}
          Recall: {avg_scores.get('recall', 'N/A')}
          Groundedness: {avg_scores.get('groundedness', 'N/A')}
          Hallucination: {avg_scores.get('hallucination_score', 'N/A')}
          Latency: {perf.get('latency_total', 'N/A')}ms
          Cost/Query: ${perf.get('cost_per_query', 'N/A')}
                  """
              except Exception as e:
                  metrics_summary = f"Error loading metrics: {e}"
          
          qg_path = pathlib.Path('chat_pipeline/evaluation/eval_results/quality_gate_result.json')
          if qg_path.exists():
              with open(qg_path, 'r') as f:
                  qg_data = json.load(f)
                  qg_status = qg_data.get('status', 'N/A')
          
          body = f"""FrontShiftAI CI/CD Pipeline Summary
          
          Repository: {os.getenv('GITHUB_REPOSITORY', 'N/A')}
          Branch: {os.getenv('GITHUB_REF_NAME', 'N/A')}
          Run ID: {os.getenv('GITHUB_RUN_ID', 'N/A')}
          Commit: {os.getenv('GITHUB_SHA', 'N/A')[:7]}
          
          JOB STATUS
          - Setup & Validation:  {setup.upper()}
          - Run Evaluation:      {evaluation.upper()}
          - Quality Gate Check:  {quality_gate.upper()}
          
          Overall Status: {overall}
          
          EVALUATION METRICS
          {metrics_summary}
          
          Quality Gate Status: {qg_status}
          
          View full details:
          {os.getenv('GITHUB_RUN_URL', 'N/A')}
          
          Automated notification from FrontShiftAI CI/CD Pipeline
          """
          
          sender = os.getenv('EMAIL_SENDER')
          password = os.getenv('EMAIL_PASSWORD')
          receiver = os.getenv('EMAIL_RECEIVER')
          
          if not all([sender, password, receiver]) or 'placeholder' in sender.lower():
              print('Email credentials not configured. Skipping notification.')
              exit(0)
          
          msg = MIMEMultipart()
          msg['From'] = sender
          msg['To'] = receiver
          msg['Subject'] = subject
          msg.attach(MIMEText(body, 'plain'))
          
          try:
              with smtplib.SMTP_SSL('smtp.gmail.com', 465, timeout=15) as server:
                  server.login(sender, password)
                  server.send_message(msg)
              print(f'Email sent to {receiver}')
          except Exception as e:
              print(f'Email failed: {e}')
              exit(0)
          EOF
