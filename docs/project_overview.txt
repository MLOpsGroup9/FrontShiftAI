FrontShiftAI – Complete Project Description

The user interacts with the UI through chat and voice.

There are two interaction modes:

Text input → Text output

Voice input → Voice output

Purpose

FrontShiftAI is designed to help HR professionals by acting as an intelligent assistant.

A company’s policy handbook (which includes information such as leave policies, working hours, etc.) will be uploaded into the system.
Employees can then ask questions related to these policies.

Additional Features
1. User Login

Each user must log in to use the platform.

If a user is new to the company, their homepage will include a short test or assessment based on the policy manual.

The homepage will have a section dedicated to these assessments.

(Note: Since this is a semester project and time is limited, the assessment feature may be skipped or left incomplete.)

2. Chatting Feature (Main Functionality)

The main feature of the application is the chat system, where users can:

Ask HR-related questions.

Interact with the assistant through text or voice.

Additionally, the assistant should be able to perform tasks like:

Checking how many leaves the user currently has (by reading from their calendar).

Applying for a leave when requested (for example: “Apply for leave from October 12 to October 15.”).

To enable this functionality, we plan to integrate an agent that can access the user’s calendar and handle leave requests.

Technical Setup

We are using a RAG (Retrieval-Augmented Generation) system powered by Llama 3, which we already have.

Our dataset includes around 20 PDFs from multiple domains and companies.
These PDFs will be used as the knowledge base for the HR assistant.

Process:

Extract text and tables from the PDFs using a tool like Docling.

Store the extracted data as embeddings in a vector database (e.g., ChromaDB or Pinecone).

Once embeddings are stored, Llama 3 will generate question–answer–context pairs for fine-tuning.

Questions
Q1.

Is it possible for Llama 3 to generate Q&A pairs since the knowledge is grounded in the uploaded documents?

Q2.

The PDFs we are extracting from contain both text and tables.
When storing them in the database, should I combine the tables with the text (by converting them to Markdown or plain text), or should I store them separately?
What are the pros and cons of both approaches?

Q3.

When fine-tuning, the Q&A pairs will include information from multiple documents.

For example, the number of annual leave days differs from company to company.
Will this cause the model to hallucinate or mix up information during fine-tuning?

If so, how can I prevent that?
Should I fine-tune the model on just a single document?
Or should I only train the LLM (Llama 3) on how to structure and answer questions, not on the factual content?

For the demo, should we use multiple documents in the database, or just one to avoid conflicting answers?

Future Product Goal

In the future, we want to turn this into a product.

When a company wants to try it, they should be able to upload their own policy document, and the system should work only with their data for their employees.

During the demo, however, I don’t want the model to give 10 different answers to the same question (like “How many annual leaves do I have?”) because we’re using documents from multiple companies.

So, I need to figure out the correct approach for:

Fine-tuning strategy

Data storage

Handling company-specific policies

Additional Note

I don’t want the system to output tables when answering user queries — only text.
So, it’s fine to store tables as text in the database and not preserve the table structure.









FrontShiftAI – HR Copilot Project Overview
1. Project Concept

FrontShiftAI is an intelligent HR assistant designed to help employees and HR professionals interact with company policies in a conversational manner.
The system supports both text and voice interactions — enabling users to ask questions or perform actions naturally.

Interaction Modes

Text Input → Text Output

Voice Input → Voice Output

The assistant focuses on understanding company policies (e.g., leave, benefits, working hours, etc.) and answering user questions based on those documents.

2. Core Features
(A) Company Policy Understanding

HR departments can upload their company’s policy handbook (in PDF format).

The assistant extracts and processes these documents to answer questions such as:

“What is the company’s leave policy?”

“How many sick leaves do I get in a year?”

“What is the notice period before resignation?”

The model uses a Retrieval-Augmented Generation (RAG) pipeline built on Llama 3, combined with a vector database (like ChromaDB or Pinecone) to retrieve relevant chunks from the uploaded documents.

(B) User Login System

Each user logs in with their company credentials.
After login, two main pages are available:

Home Page (for new employees)

Displays a welcome section.

Optionally includes a policy assessment/test on the uploaded manual.

(Note: For this semester, the assessment feature is secondary and may be skipped.)

Chat Page (Main Feature)

Employees can chat with the HR assistant directly.

The chat interface supports both text and voice modes.

(C) Leave Management via Agent

We plan to implement an agent-based system to handle user actions related to leaves.

Example interactions:

“How many leaves do I have left?”

The agent checks the user’s calendar and calculates available leave balance.

“Apply for a leave from October 15 to October 20.”

The agent updates the calendar and confirms the request.

This feature integrates calendar APIs and user data to automate leave tracking and management.

3. Technical Workflow

Data Source: ~20 PDF policy documents from different companies and domains.

Extraction: Using Docling to extract both text and tables from the PDFs.

Storage: Embeddings generated from text chunks and stored in ChromaDB or Pinecone.

Retrieval: Llama 3 uses RAG to retrieve relevant policy sections when users ask questions.

Fine-tuning (optional): Llama 3 can be fine-tuned using synthetic Q&A + context pairs generated from the extracted data.

4. Key Questions
Q1. Can Llama 3 generate Q&A pairs from grounded knowledge?

✅ Yes, absolutely.

Llama 3 can generate question–answer pairs if you provide it with context chunks from your company policy documents.

Recommended process:

Feed one chunk of text at a time (e.g., 300–500 tokens).

Prompt the model:
“Generate 2–3 question–answer pairs strictly based on the text below. Do not invent any facts. Output as JSON.”

Validate each generated answer to ensure it only uses information from the given chunk.

This creates a high-quality fine-tuning dataset grounded entirely in real company policies.

Q2. Should text and tables be stored together or separately in the database?

👉 Recommended: Convert tables to plain text and store them together with normal text.

Option A – Combine (Recommended)

Convert each table row into readable text sentences (e.g., “Leave Type: Sick Leave, Limit: 7 days per year.”)

Store combined text in the vector DB.

Pros:

Simplifies retrieval and model input.

Maintains consistency across all document types.

Avoids showing raw tables to users.

Cons:

Loses exact table layout (minor issue for your use case).

Option B – Store Separately

Keep tables as JSON or CSV entries in a separate index.

Pros:

Useful for advanced analytics or structured queries later.
Cons:

Increases system complexity.

Requires separate retrieval and formatting logic.

Since you don’t want to display tables in responses, Option A is the best fit.

Q3. Fine-tuning Strategy (Avoiding Hallucinations Across Companies)

If you fine-tune Llama 3 on data from multiple companies, you risk cross-document interference — for example:

Q: “How many annual leaves do I get?”
A: “7 days” (from Company A) OR “14 days” (from Company B)

The model might memorize conflicting answers and mix them up.

Solution

Use Fine-tuning for Behavior, Not Facts

Train the model on how to answer (format, tone, citation) — not what to answer.

Keep factual data in your vector database and retrieve dynamically.

Scope by Company (Tenant Isolation)

Store documents per company (separate DB collections or namespaces).

During retrieval, filter by company_id.

This ensures each user only gets answers from their company.

For Demo

Use one company’s data to avoid conflicting answers.

This ensures consistency and clarity.

For Real Deployment

When a company signs up, only their documents are embedded and stored in their private namespace.

Each user login automatically filters retrieval to that namespace.

Summary Table
Topic	Recommendation
QnA Generation	Use Llama 3 with grounded chunks (no hallucination)
Tables	Convert to readable text, store with text
Fine-tuning	Focus on answer style, not facts
Multi-company	Separate vector DBs / namespaces
Demo	Single company dataset
Product	Company uploads docs → embeddings stored → isolated retrieval
Tables in Output	Exclude from output; use plain text format